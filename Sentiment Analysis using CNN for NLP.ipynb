{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EJ6zhycx-LJa"
   },
   "source": [
    "## Importing dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NON9giQ1_eZy"
   },
   "outputs": [],
   "source": [
    "#re is an important module for cleaning data \n",
    "#bs4 is important to read data \n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "#google drive is an optional addition \n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Un7jTbQNedR6"
   },
   "outputs": [],
   "source": [
    "#try and except is hereby used to select the correct version of tensorflow\n",
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WN8EilQG-cwa"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "52CTV4_q-hpX"
   },
   "source": [
    "### Loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "solg5Rzx6-oY",
    "outputId": "defe36bd-2945-4470-b252-899d3666c4e2"
   },
   "outputs": [],
   "source": [
    "''' \n",
    "to mount the google drive to your colab notebook\n",
    "'''\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X5Qkr_1Zfgi9"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This is with regards to the data we have from the sentiment140 website.\n",
    "\n",
    "In latin1 each character is exactly one byte long. In utf8 a character can consist of more than one byte.\n",
    "Consequently utf8 has more characters than latin1(and the characters they do have \n",
    "in common aren't necessarily represented by the same byte/bytesequence).\n",
    "\n",
    "engine='python' is added to avoid parser warning while using Google colab.\n",
    "\n",
    "'''\n",
    "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
    "train_data = pd.read_csv(\n",
    "    \"/content/drive/My Drive/CNN for NLP/Data/train.csv\",\n",
    "    header=None,\n",
    "    names=cols,\n",
    "    engine=\"python\",\n",
    "    encoding=\"latin1\"\n",
    ")\n",
    "test_data = pd.read_csv(\n",
    "    \"/content/drive/My Drive/CNN for NLP/Data/test.csv\",\n",
    "    header=None,\n",
    "    names=cols,\n",
    "    engine=\"python\",\n",
    "    encoding=\"latin1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "colab_type": "code",
    "id": "NJYt1sa_KH4P",
    "outputId": "72f7f490-1f61-4096-dff7-c06d03d43d01"
   },
   "outputs": [],
   "source": [
    "train_data.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M5Md2zOM-SPH"
   },
   "source": [
    "The test dataset has 3 different labels (a negative, a positive and a neutral one) while the train dataset has only two so we will not use the test file, and split the train file later by ourselves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-6BBPb3-OfY"
   },
   "outputs": [],
   "source": [
    "data = train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6CsZKJx1-2Ep"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FLWayJ-5-7nN"
   },
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qa1v91RSkgz1"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The columns mentioned are not required for our model as they are not related to sentiment analysis\n",
    "in our following data.\n",
    "\n",
    "inplace=True so that the data gets rewritten with the changes\n",
    "axis=1 so that it selects it for columns( axis=0 for rows)\n",
    "'''\n",
    "data.drop([\"id\", \"date\", \"query\", \"user\"],\n",
    "          axis=1,\n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qU-7WW0m9O5j"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The data we have must be cleaned as a whole before processing it for training the model we have.\n",
    "\n",
    "For this, we need to use the BeautifulSoup and then the RegEx modules are used.\n",
    "\n",
    "The BeautifulSoup-lxml relation: https://stackoverflow.com/questions/27790415/set-lxml-as-default-beautifulsoup-parser\n",
    "is a web parser, but here it can also parse through our data and get text foreach line, which we have done here.\n",
    "\n",
    "Second, we remove all the mentions, using @ by implementing regex as (r\"@[A-Za-z0-9]+,' ',tweet) this means that remove the @\n",
    "and its accompanying text and do it repeatedly replacing it by whitespaces in the tweets.\n",
    "\n",
    "Third, the URL links which might be present inside the tweets have to be removed. It's removed by (r\"https?://[A-Za-z0-9./]+,' ', tweet)\n",
    "which means remove https links with all the characters mentioned inside the square brackets and do it repeatedly and replace by whitespaces.\n",
    "\n",
    "Fourth, the letters are kept using the hat symbol ('^') inside the regex. Here, + is not required.\n",
    "\n",
    "Then, the extra whitespaces are removed using the same regex, here being, (r\" +\", \" \", tweet).\n",
    "All of these require re.sub which is substitute attribute of the regex module.\n",
    "'''\n",
    "def clean_tweet(tweet):\n",
    "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
    "    # Removing the @\n",
    "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
    "    # Removing the URL links\n",
    "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
    "    # Keeping only letters\n",
    "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n",
    "    # Removing additional whitespaces\n",
    "    tweet = re.sub(r\" +\", ' ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vq-mIZNdAUjE"
   },
   "outputs": [],
   "source": [
    "#Using list comprehensions the cleaned data is saved in data_clean for every tweet in the data.\n",
    "data_clean = [clean_tweet(tweet) for tweet in data.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TqtCJZkhAb9C"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "After cleaning the data there might be different labels for the sentiments that are present.\n",
    "We want it to 0 or1 as they need to be positive or negative. So, value 4 is changed to 1.\n",
    "'''\n",
    "data_labels = data.sentiment.values\n",
    "data_labels[data_labels == 4] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GTKZ5fUh_Kxz"
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IvmIKnAnAJRY"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The tokenizer does the numbering of different words in the data_clean and then makes a vocabulary of 2**17 words,\n",
    "using the TensorFlow_Datasets features, which has a Subword Text Encoder to build this corpus.\n",
    "'''\n",
    "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    data_clean, target_vocab_size=2**17\n",
    ")\n",
    "\n",
    "data_inputs = [tokenizer.encode(sentence) for sentence in data_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ysb2uib8n6b3"
   },
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M9qttbt7BMwg"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Now, we pad the inputs so that all the sentences which are already encoded using the tokenizer.\n",
    "For this we find the maximum length of the longest sentence in the data and then add the number of zeroes \n",
    "at the end to make every sentence of the same length.\n",
    "We do padding so that the processes are unbiased.\n",
    "'''\n",
    "MAX_LEN = max([len(sentence) for sentence in data_inputs])\n",
    "data_inputs = tf.keras.preprocessing.sequence.pad_sequences(data_inputs,\n",
    "                                                            value=0,\n",
    "                                                            padding=\"post\",\n",
    "                                                            maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T4Ac7EXNNblp"
   },
   "source": [
    "### Spliting into training/testing set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x_H7zROsNbCE"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Now we must divide our data set to training and testing, wherein our data set has around 1600000 inouts,\n",
    "half is divided to test and the other 80k into training data.\n",
    "Then, in the next cells test inputs and test labels are segregated.\n",
    "'''\n",
    "test_idx = np.random.randint(0, 800000, 8000)\n",
    "test_idx = np.concatenate((test_idx, test_idx+800000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XT7vDPUb06lw"
   },
   "outputs": [],
   "source": [
    "test_inputs = data_inputs[test_idx]\n",
    "test_labels = data_labels[test_idx]\n",
    "train_inputs = np.delete(data_inputs, test_idx, axis=0)\n",
    "train_labels = np.delete(data_labels, test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWu6hLDG_UJZ"
   },
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fD3nbD_M94Gt"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This is the model building cell which does the work of analysis after it is fed with data.\n",
    "A class DCNN is formed with the main attribute as t.keras.Model.\n",
    "\n",
    "First, using __init__ function , and self being the mandatory attribute, various other attributes \n",
    "are defined with initial values.\n",
    "\n",
    "emb_dim is the size of the vectors that are used for embedding in 1D. Here, it is 128 bits.\n",
    "\n",
    "vocab_size is given through the tokenizer which makes a word corpus of size 2**17.\n",
    "\n",
    "nb_filters are the number of filters that need to be applied for convolution.\n",
    "\n",
    "nb_classes is naive bayes text classification \n",
    "\n",
    "dropout_rate is basically the ratio to define how many of the inputs are excluded in the update cycle.\n",
    "so, a dropout_rate of 0.1 means 1 out of 10 inouts are excluded.\n",
    "'''\n",
    "class DCNN(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 emb_dim=128,\n",
    "                 nb_filters=50,\n",
    "                 FFN_units=512,\n",
    "                 nb_classes=2,\n",
    "                 dropout_rate=0.1,\n",
    "                 training=False,\n",
    "                 name=\"dcnn\"):\n",
    "        super(DCNN, self).__init__(name=name)\n",
    "        '''\n",
    "        Embedding is for the flattening of the layers we provide initially.\n",
    "        \n",
    "        \"The kernel size here refers to the widthxheight of the filter mask. The max pooling layer, for example, \n",
    "        returns the pixel with maximum value from a set of pixels within a mask (kernel).\n",
    "        That kernel is swept across the input, subsampling it.\"\n",
    "        \n",
    "        Activation Functions: https://towardsdatascience.com/activation-functions-and-its-types-which-is-better-a9a5310cc8f\n",
    "        Basically, they create the scope of the function to be a nonlinear function, here ReLU is Rectified Linear Unit\n",
    "        \n",
    "        dense basically concatenates all the layers after the pool_3 \n",
    "        \n",
    "        again one more densing is done using last dense using different activation functions such as sigmoid or softmax.\n",
    "        \n",
    "        lastly, the attributes are called using call function and then the results are merged using concatenation of \n",
    "        x_1 x_2 and x_3.\n",
    "        '''\n",
    "        self.embedding = layers.Embedding(vocab_size,\n",
    "                                          emb_dim)\n",
    "        self.bigram = layers.Conv1D(filters=nb_filters,\n",
    "                                    kernel_size=2,\n",
    "                                    padding=\"valid\",\n",
    "                                    activation=\"relu\")\n",
    "        self.pool_1=layers.GlobalMaxPool1D()\n",
    "        self.trigram = layers.Conv1D(filters=nb_filters,\n",
    "                                     kernel_size=3,\n",
    "                                     padding=\"valid\",\n",
    "                                     activation=\"relu\")\n",
    "        self.pool_2=layers.GlobalMaxPool1D()\n",
    "        self.fourgram = layers.Conv1D(filters=nb_filters,\n",
    "                                      kernel_size=4,\n",
    "                                      padding=\"valid\",\n",
    "                                      activation=\"relu\")\n",
    "        self.pool_3 = layers.GlobalMaxPool1D() # no training variable so we can\n",
    "                                             # use the same layer for each\n",
    "                                             # pooling step\n",
    "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        if nb_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=1,\n",
    "                                           activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=nb_classes,\n",
    "                                           activation=\"softmax\")\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        x = self.embedding(inputs)\n",
    "        x_1 = self.bigram(x)\n",
    "        x_1 = self.pool_1(x_1)\n",
    "        x_2 = self.trigram(x)\n",
    "        x_2 = self.pool_2(x_2)\n",
    "        x_3 = self.fourgram(x)\n",
    "        x_3 = self.pool_3(x_3)\n",
    "        \n",
    "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n",
    "        merged = self.dense_1(merged)\n",
    "        merged = self.dropout(merged, training)\n",
    "        output = self.last_dense(merged)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "92XbAZ9E1AMS"
   },
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B8cfYwHME-m0"
   },
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YXwGD-pqFG4n"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "\n",
    "EMB_DIM = 256\n",
    "NB_FILTERS = 128\n",
    "FFN_UNITS = 256\n",
    "NB_CLASSES = len(set(train_labels))\n",
    "\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NB_EPOCHS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nddzr1kA7UHC"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ETcf5Wl4Q-7"
   },
   "outputs": [],
   "source": [
    "Dcnn = DCNN(vocab_size=VOCAB_SIZE,\n",
    "            emb_dim=EMB_DIM,\n",
    "            nb_filters=NB_FILTERS,\n",
    "            FFN_units=FFN_UNITS,\n",
    "            nb_classes=NB_CLASSES,\n",
    "            dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XCuNhMNk4n_u"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Here, the three important functions forcompiling a model using the values that are provided are,\n",
    "1) Optimizer: https://www.dlology.com/blog/quick-notes-on-how-to-choose-optimizer-in-keras/\n",
    "              It is an algorithm that decides the path the code will take to go through to the final result.\n",
    "              Adam or SGD are preferred ones\n",
    "2) Loss: https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/\n",
    "         A scalar value that we attempt to minimize during our training of the model.\n",
    "         The lower the loss, the closer our predictions are to the true labels.\n",
    "3) Metrics: https://machinelearningmastery.com/custom-metrics-deep-learning-keras-python/\n",
    "            https://stackoverflow.com/a/47306502\n",
    "'''\n",
    "if NB_CLASSES == 2:\n",
    "    Dcnn.compile(loss=\"binary_crossentropy\",\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=[\"accuracy\"])\n",
    "else:\n",
    "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1X7h6Bx5Upc"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./drive/My Drive/projects/CNN_for_NLP/ckpt/\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Latest checkpoint restored!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "6eL9EMsa6Igy",
    "outputId": "702c8976-2d50-4fe7-a9f8-1bf5317567cb"
   },
   "outputs": [],
   "source": [
    "Dcnn.fit(train_inputs,\n",
    "         train_labels,\n",
    "         batch_size=BATCH_SIZE,\n",
    "         epochs=NB_EPOCHS)\n",
    "ckpt_manager.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16Gn6JhJKXDK"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Jt2dRZWhKHbT",
    "outputId": "a6665206-cf11-4e2d-8c5d-b544de30e902"
   },
   "outputs": [],
   "source": [
    "results = Dcnn.evaluate(test_inputs, test_labels, batch_size=BATCH_SIZE)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "te0SJTWe2EVb",
    "outputId": "54dd9575-d2e8-4e9c-e820-1d105a5f2886"
   },
   "outputs": [],
   "source": [
    "Dcnn(np.array([tokenizer.encode(\"you're fat\")]), training=False).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TjInCgj24pp7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-bAxtCk2Y-m"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "B8cfYwHME-m0",
    "Nddzr1kA7UHC",
    "16Gn6JhJKXDK"
   ],
   "name": "Simple_CNN_for_NLP_udemy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
